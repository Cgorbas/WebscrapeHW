{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, redirect\n",
    "from flask_pymongo import PyMongo\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.mars_db\n",
    "collection = db.mars_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "NASA Garners 7 Webby Award Nominations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NASA's Opportunity Rover Mission on Mars Comes to End\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NASA's InSight Places First Instrument on Mars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NASA Announces Landing Site for Mars 2020 Rover\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Opportunity Hunkers Down During Dust Storm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NASA Finds Ancient Organic Material, Mysterious Methane on Mars\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_titles = soup.find_all('div', class_= 'content_title')\n",
    "\n",
    "news_title=[]\n",
    "\n",
    "for n_title in n_titles:\n",
    "    if (n_title.a):\n",
    "        if (n_title.a.text):\n",
    "            news_title.append(n_title)  \n",
    "for x in range(len(news_title)):\n",
    "    print(news_title[x].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nominees include four JPL projects: the solar system and climate websites, InSight social media, and a 360-degree Earth video. Public voting closes April 18, 2019.\n",
      "\n",
      "\n",
      "NASA's Opportunity Mars rover mission is complete after 15 years on Mars. Opportunity's record-breaking exploration laid the groundwork for future missions to the Red Planet.\n",
      "\n",
      "\n",
      "In deploying its first instrument onto the surface of Mars, the lander completes a major mission milestone.\n",
      "\n",
      "\n",
      "After a five-year search, NASA has chosen Jezero Crater as the landing site for its upcoming Mars 2020 rover mission.\n",
      "\n",
      "\n",
      "It's the beginning of the end for the planet-encircling dust storm on Mars. But it could still be weeks, or even months, before skies are clear enough for NASA's Opportunity rover to recharge its batteries and phone home. \n",
      "\n",
      "\n",
      "NASA’s Curiosity rover has found evidence on Mars with implications for NASA’s search for life.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_ps = soup.find_all('div', class_= 'rollover_description_inner')\n",
    "\n",
    "news_p=[]\n",
    "\n",
    "for n_p in n_ps:\n",
    "    if (n_p):\n",
    "        if (n_p.text):\n",
    "            news_p.append(n_p)  \n",
    "\n",
    "for x in range(len(newsp)):\n",
    "    print(news_p[x].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsp=news_p newstitles=news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x2e873b9fcc8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " collection.insert_one(mars_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the MongoDB records created above\n",
    "articles = db.articles.find()\n",
    "for article in articles:\n",
    "    print(article + \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-85e59e31686a>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-85e59e31686a>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    mars_news[\"news_title\"] = soup.find(\"div\", class =\"content_title\").get_text()\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape():\n",
    "    browser = init_browser()\n",
    "    mars_news = {}\n",
    "\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    mars_news[\"news_title\"] = soup.find(\"div\", class =\"content_title\").get_text()\n",
    "    mars_news[\"news_p\"] = soup.find(\"div\", class =\"article_teaser_body\").get_text()\n",
    "\n",
    "\n",
    "    return mars_news\n",
    "\n",
    "print(mars_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the parent divs for all articles\n",
    "results = soup.find_all('li', class_='mixed-feed__item--article')\n",
    "\n",
    "# Loop through results to retrieve article title, header, and timestamp of article\n",
    "for result in results:\n",
    "    title = result.find('h4', class_='mixed-feed__header').text\n",
    "\n",
    "    lede = result.find('h5', class_='mixed-feed__subheader').text\n",
    "\n",
    "    # The time and date of article publication\n",
    "    date = result.find('time')['datetime']\n",
    "    # Slice the datetime string for the date\n",
    "    article_date = date[:10]\n",
    "    # Slice the datetime string for the time\n",
    "    time = date[11:16]\n",
    "    # Determine whether article was published in AM or PM\n",
    "    if (int(time[:2]) >= 13):\n",
    "        meridiem = 'pm'\n",
    "    else:\n",
    "        meridiem = 'am'\n",
    "\n",
    "    # Concatenate time string\n",
    "    time = time + meridiem\n",
    "    print('-----------------')\n",
    "    print(title)\n",
    "    print(lede)\n",
    "    print(article_date)\n",
    "    print(time)\n",
    "\n",
    "    # Dictionary to be inserted into MongoDB\n",
    "    post = {\n",
    "        'title': title,\n",
    "        'lede': lede,\n",
    "        'date': article_date,\n",
    "        'time published': time\n",
    "    }\n",
    "\n",
    "    # Insert dictionary into MongoDB as a document\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function scrape at 0x00000147831BED90>\n"
     ]
    }
   ],
   "source": [
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": r\"C:\\Users\\CopperSun\\Documents\\chromedriver_win32\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "def scrape_info():\n",
    "    browser = init_browser()\n",
    "\n",
    "    # Visit visitcostarica.herokuapp.com\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    \n",
    "def scrape():\n",
    "    browser = init_browser()\n",
    "    mars_news = {}\n",
    "\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    mars_news[\"headline\"] = soup.find(\"a\", class_=\"result-title\").get_text()\n",
    "    mars_news[\"price\"] = soup.find(\"span\", class_=\"result-price\").get_text()\n",
    "\n",
    "\n",
    "    return mars_news\n",
    "\n",
    "print(scrape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mongo\n",
    "# Create an instance of Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Use PyMongo to establish Mongo connection\n",
    "mongo = PyMongo(app, uri=\"mongodb://localhost:27017/weather_app\")\n",
    "\n",
    "\n",
    "# Route to render index.html template using data from Mongo\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "\n",
    "    # Find one record of data from the mongo database\n",
    "    destination_data = mongo.db.collection.find_one()\n",
    "\n",
    "    # Return template and data\n",
    "    return render_template(\"index.html\", vacation=destination_data)\n",
    "\n",
    "\n",
    "# Route that will trigger the scrape function\n",
    "@app.route(\"/scrape\")\n",
    "def scrape():\n",
    "\n",
    "    # Run the scrape function\n",
    "    costa_data = scrape_costa.scrape_info()\n",
    "\n",
    "    # Update the Mongo database using update and upsert=True\n",
    "    mongo.db.collection.update({}, costa_data, upsert=True)\n",
    "\n",
    "    # Redirect back to home page\n",
    "    return redirect(\"/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
